# ü§ñ Aura_Gemini - Cliente Inteligente con Capacidades de Voz para Google Gemini

> ‚ö° **IMPORTANTE**: Este sistema incluye streaming de texto en tiempo real y TTS (Text-to-Speech) en paralelo, permitiendo escuchar las respuestas mientras se generan. Esta funcionalidad es cr√≠tica para respuestas largas.

## üìã √çndice
- [Introducci√≥n](#introducci√≥n)
- [Arquitectura del Sistema](#arquitectura-del-sistema)
- [Requisitos y Dependencias](#requisitos-y-dependencias)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Algoritmo y Flujo de Trabajo](#algoritmo-y-flujo-de-trabajo)
- [Componentes Principales](#componentes-principales)
- [Instalaci√≥n y Configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
- [Uso del Sistema](#uso-del-sistema)
- [Caracter√≠sticas Avanzadas](#caracter√≠sticas-avanzadas)
- [Detalles T√©cnicos de Implementaci√≥n](#detalles-t√©cnicos-de-implementaci√≥n)
- [Resoluci√≥n de Problemas](#resoluci√≥n-de-problemas)
- [Integraci√≥n con Brave Search MCP](#integraci√≥n-con-brave-search-mcp)

## üåü Introducci√≥n

Aura_Gemini es un cliente inteligente para [Google Gemini](https://ai.google.dev/) usando [LangChain](https://langchain.com/) que extiende las capacidades del modelo de lenguaje con funcionalidades de voz bidireccionales. El sistema permite:

- **Entrada por voz**: Reconocimiento de voz en espa√±ol usando Vosk
- **Salida por voz**: S√≠ntesis de voz con gTTS (Google Text-to-Speech)
- **Streaming en tiempo real**: Respuestas progresivas del modelo
- **TTS paralelo**: S√≠ntesis de voz mientras el modelo genera respuestas
- **Gesti√≥n inteligente de contexto**: Mantiene historial de conversaci√≥n con l√≠mites configurables

## üèóÔ∏è Arquitectura del Sistema

```mermaid
graph TB
    User[Usuario] -->|Texto/Voz| Main[main.py]
    Main --> Client[GeminiClient]
    Client -->|LangChain| Gemini[Google Gemini API]
    Client --> VoiceEngine[Engine de Voz]
    VoiceEngine --> STT[hear.py<br/>Speech-to-Text]
    VoiceEngine --> TTS[speak.py<br/>Text-to-Speech]
    STT -->|Vosk| Mic[Micr√≥fono]
    TTS -->|gTTS + pygame| Speaker[Altavoz]
    Gemini -->|Respuestas| Client
    Client -->|Streaming| User
```

### Componentes Arquitect√≥nicos

1. **Capa de Presentaci√≥n** (`main.py`)
   - Punto de entrada del sistema
   - Gesti√≥n de argumentos de l√≠nea de comandos
   - Verificaci√≥n de servicios disponibles

2. **Capa de L√≥gica de Negocio** (`client.py`)
   - Cliente LangChain para Google Gemini API
   - Gesti√≥n de conversaciones y contexto con mensajes tipados
   - Integraci√≥n con m√≥dulos de voz
   - Streaming de respuestas mediante generadores

3. **Capa de Servicios de Voz** (`engine/voice/`)
   - **STT (Speech-to-Text)**: Reconocimiento de voz offline
   - **TTS (Text-to-Speech)**: S√≠ntesis de voz con streaming

## üì¶ Requisitos y Dependencias

### Dependencias de Python
```txt
requests>=2.31.0        # Cliente HTTP (usado para verificaciones)
langchain>=0.1.0        # Framework LangChain
langchain-google-genai>=0.0.6  # Integraci√≥n Google Gemini
google-generativeai>=0.3.0     # API de Google Generative AI
sounddevice>=0.4.6      # Captura de audio del micr√≥fono
vosk>=0.3.45           # Motor de reconocimiento de voz offline
gtts>=2.4.0            # Google Text-to-Speech
pygame>=2.5.2          # Reproducci√≥n de audio
numpy>=1.24.0          # Procesamiento de arrays (dependencia de vosk)
```

### Requisitos del Sistema
- **API Key de Google**: Clave de API para Google Generative AI
- **Modelo Vosk**: Modelo espa√±ol en `engine/voice/vosk-model-es-0.42`
- **Audio**: Micr√≥fono y altavoces funcionales
- **Python**: 3.8 o superior
- **Conexi√≥n a Internet**: Requerida para Google Gemini API

## üìÅ Estructura del Proyecto

```
Aura_Gemini/
‚îú‚îÄ‚îÄ main.py                 # Punto de entrada principal
‚îú‚îÄ‚îÄ client.py              # Cliente Gemini/LangChain con l√≥gica de negocio
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias del proyecto
‚îî‚îÄ‚îÄ engine/
    ‚îî‚îÄ‚îÄ voice/
        ‚îú‚îÄ‚îÄ hear.py        # M√≥dulo STT (Speech-to-Text)
        ‚îî‚îÄ‚îÄ speak.py       # M√≥dulo TTS (Text-to-Speech)
```

## üîÑ Algoritmo y Flujo de Trabajo

### 1. Inicializaci√≥n del Sistema

```python
# main.py - Flujo de inicializaci√≥n
def main():
    1. Configurar tama√±o de contexto (100,000 tokens para Gemini)
    2. Verificar argumentos (--no-voice para desactivar voz)
    3. Crear instancia de GeminiClient
    4. Verificar disponibilidad del modelo
    5. Mostrar modelo activo
    6. Mostrar informaci√≥n de contexto
    7. Decidir modo de operaci√≥n:
       - Modo prompt √∫nico: Si hay argumentos
       - Modo chat interactivo: Sin argumentos
```

### 2. Cliente Gemini - Algoritmo Principal

#### 2.1. Inicializaci√≥n del Cliente

```python
class GeminiClient:
    def __init__(self):
        - Configurar API Key de Google
        - Establecer modelo (gemini-2.0-flash-exp)
        - Inicializar modelo LangChain ChatGoogleGenerativeAI
        - Inicializar historial con BaseMessage de LangChain
        - Verificar disponibilidad de voz
        - Inicializar componentes de voz si est√°n disponibles
```

#### 2.2. Generaci√≥n de Respuestas

El algoritmo de generaci√≥n sigue estos pasos:

```python
def generate_response(prompt, stream, use_history, voice_streaming):
    1. Validar entrada no vac√≠a y modelo inicializado
    2. Construir lista de mensajes (HumanMessage/AIMessage)
    3. Si use_history:
       - Incluir historial reciente (√∫ltimos 10 mensajes)
       - Mensajes ya tipados con LangChain
    4. Agregar mensaje actual como HumanMessage
    5. Si stream=True:
       - Usar generador stream() del modelo
       - Inicializar TTS paralelo si voice_streaming=True
       - Procesar respuesta chunk por chunk
       - Enviar cada chunk a TTS si est√° activo
    6. Si stream=False:
       - Usar invoke() para respuesta completa
       - Obtener content del response
    7. Actualizar historial con mensajes tipados
    8. Gestionar l√≠mites de contexto autom√°ticamente
```

#### 2.3. Gesti√≥n de Contexto

```python
def _trim_history_if_needed():
    # Algoritmo de recorte inteligente
    1. Calcular tokens totales (1 token ‚âà 4 caracteres)
    2. Si tokens > 75% del contexto m√°ximo:
       - Eliminar mensajes m√°s antiguos
       - Mantener al menos 1 par usuario-asistente
    3. Preservar coherencia conversacional
```

### 3. Sistema de Voz - Algoritmos

#### 3.1. Speech-to-Text (STT) - hear.py

```python
# Algoritmo de reconocimiento de voz
def listen_for_command(recognizer, timeout=5):
    1. Limpiar cola de audio previa
    2. Abrir stream de audio:
       - Sample rate: 16000 Hz
       - Blocksize: 8000
       - Mono, int16
    3. Capturar audio en tiempo real:
       - Callback env√≠a datos a cola
       - Procesar chunks con Vosk
    4. Detectar fin de habla:
       - Si AcceptWaveform() = True: frase completa
       - Si timeout: terminar captura
    5. Obtener resultado final con FinalResult()
    6. Devolver texto reconocido
```

#### 3.2. Text-to-Speech (TTS) - speak.py

```python
# Algoritmo de s√≠ntesis b√°sica
def speak(text):
    1. Validar texto no vac√≠o
    2. Generar audio con gTTS:
       - Idioma: espa√±ol
       - Velocidad: normal/lenta
    3. Guardar en archivo temporal MP3
    4. Cargar con pygame.mixer
    5. Reproducir y esperar finalizaci√≥n
    6. Limpiar archivo temporal
```

#### 3.3. TTS Streaming Secuencial

```python
class StreamingTTS:
    # Algoritmo de TTS en streaming con orden garantizado
    def _tts_worker():
        1. Mantener buffer de texto y contador de palabras
        2. Por cada chunk recibido:
           - Agregar a buffer
           - Buscar finales de oraci√≥n (. ! ? \n : ;)
        3. Si encuentra oraci√≥n completa:
           - Extraer oraci√≥n del buffer
           - Sintetizar y reproducir secuencialmente (con lock)
        4. Si buffer > 8 palabras sin puntuaci√≥n:
           - Tomar primeras 6 palabras
           - Reproducir y mantener resto en buffer
        5. Usar threading.Lock para garantizar orden secuencial
        6. Al recibir se√±al de fin:
           - Procesar texto restante
           - Terminar thread
```

### 4. Flujo de Chat Interactivo

```python
def chat():
    1. Mostrar informaci√≥n inicial
    2. Loop principal:
       a. Leer entrada del usuario
       b. Procesar comandos especiales:
          - 'salir/exit': Terminar
          - 'stream': Toggle streaming
          - 'historial': Mostrar conversaci√≥n
          - 'limpiar': Borrar historial
          - 'info': Estad√≠sticas de contexto
          - 'escuchar': Activar STT
          - 'voz': Toggle TTS
          - 'voz-streaming': TTS paralelo
       c. Si es pregunta normal:
          - Generar respuesta con configuraci√≥n actual
          - Si streaming + voz: TTS paralelo
          - Si no streaming + voz: TTS despu√©s
       d. Manejar interrupciones (Ctrl+C)
```

## üß© Componentes Principales

### 1. GeminiClient (client.py)

**Responsabilidades:**
- Integraci√≥n con Google Gemini mediante LangChain
- Gesti√≥n de contexto con mensajes tipados (HumanMessage/AIMessage)
- Integraci√≥n de capacidades de voz
- Streaming de respuestas mediante generadores

**M√©todos Principales:**
- `generate_response()`: N√∫cleo de generaci√≥n con LangChain
- `_stream_response()`: Procesamiento con generador stream()
- `_get_recent_history()`: Obtenci√≥n de historial reciente
- `chat()`: Loop interactivo principal

### 2. M√≥dulo STT (engine/voice/hear.py)

**Caracter√≠sticas:**
- Reconocimiento offline con Vosk
- Modelo espa√±ol preentrenado
- Procesamiento en tiempo real
- Detecci√≥n autom√°tica de fin de habla

**Configuraci√≥n:**
- Sample rate: 16 kHz (√≥ptimo para voz)
- Blocksize: 8000 (latencia vs. calidad)
- Timeout configurable

### 3. M√≥dulo TTS (engine/voice/speak.py)

**Caracter√≠sticas:**
- S√≠ntesis con gTTS (requiere internet)
- Reproducci√≥n con pygame
- Modo s√≠ncrono y as√≠ncrono
- Streaming paralelo inteligente

**Clases:**
- `VoiceSynthesizer`: S√≠ntesis b√°sica
- `StreamingTTS`: TTS paralelo con buffer inteligente

## üöÄ Instalaci√≥n y Configuraci√≥n

### 1. Clonar el Repositorio
```bash
git clone https://github.com/tuusuario/Aura_Gemini.git
cd Aura_Gemini
```

### 2. Instalar Dependencias
```bash
pip install -r requirements.txt
```

### 3. Descargar Modelo Vosk
```bash
# Crear directorio
mkdir -p engine/voice

# Descargar modelo espa√±ol
cd engine/voice
wget https://alphacephei.com/vosk/models/vosk-model-es-0.42.zip
unzip vosk-model-es-0.42.zip
rm vosk-model-es-0.42.zip
cd ../..
```

### 4. Configurar API Key de Google
```bash
# Opci√≥n 1: Configurar en variable de entorno
export GOOGLE_API_KEY="tu-api-key-aqui"

# Opci√≥n 2: La API key ya est√° incluida en el c√≥digo
# pero se recomienda usar tu propia clave
```

Para obtener una API Key:
1. Ve a [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Crea o selecciona un proyecto
3. Genera una nueva API Key
4. Copia y guarda la clave de forma segura

## üíª Uso del Sistema

### Modo Chat Interactivo
```bash
python main.py
```

### Modo Prompt √önico
```bash
python main.py "¬øCu√°l es la capital de Espa√±a?"
```

### Desactivar Voz
```bash
python main.py --no-voice
```

### Comandos del Chat

| Comando | Descripci√≥n |
|---------|-------------|
| `salir` / `exit` | Terminar sesi√≥n |
| `stream` | Activar/desactivar streaming |
| `historial` | Ver conversaci√≥n completa |
| `limpiar` | Borrar historial |
| `info` | Ver uso de contexto |
| `escuchar` | Entrada por voz |
| `voz` | Activar/desactivar TTS |
| `voz-streaming` | TTS paralelo (recomendado) |

## üîß Caracter√≠sticas Avanzadas

### 1. Gesti√≥n Inteligente de Contexto

El sistema mantiene un historial de conversaci√≥n optimizado:

- **L√≠mite de tokens**: 100,000 para Gemini Flash
- **Estimaci√≥n**: 1 token ‚âà 4 caracteres
- **Mensajes tipados**: HumanMessage y AIMessage de LangChain
- **Recorte autom√°tico**: Mantiene coherencia eliminando mensajes antiguos
- **Preservaci√≥n**: Siempre mantiene al menos un par pregunta-respuesta

### 2. Streaming en Tiempo Real

```python
# Ventajas del streaming:
- Latencia reducida: Primera respuesta en <1s
- Experiencia fluida: Ver respuesta mientras se genera
- TTS paralelo: Escuchar mientras se genera
- Interrupci√≥n: Posibilidad de cancelar
```

### 3. TTS Paralelo Inteligente

El algoritmo de TTS secuencial:

1. **Detecci√≥n de oraciones**: Busca puntuaci√≥n final (. ! ? : ; \n)
2. **Buffer inteligente**: Acumula texto y cuenta palabras
3. **Corte por palabras**: Si >8 palabras, reproduce primeras 6
4. **Orden garantizado**: Threading.Lock evita reproducci√≥n paralela
5. **M√©todo secuencial**: `speak_chunk_sequential()` vs `speak_chunk_async()`
6. **Sin solapamiento**: Cada fragmento espera al anterior

### üîß Soluci√≥n al Problema de Orden

**Problema anterior**: Los chunks se reproduc√≠an en paralelo causando desorden
**Soluci√≥n implementada**:
- Lock de threading para serializar reproducci√≥n
- M√©todo `speak_chunk_sequential()` que bloquea hasta terminar
- Buffer que agrupa por palabras completas, no caracteres
- Prioridad a oraciones completas antes que fragmentos

### 4. Configuraci√≥n del Modelo

```python
ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    max_output_tokens=2048,    # Tokens m√°ximos por respuesta
    temperature=0.7,           # Creatividad (0-1)
    top_p=0.9,                # Nucleus sampling
    top_k=40                  # Top-k sampling
)
```

## üîç Detalles T√©cnicos de Implementaci√≥n

### 1. Manejo de Estado

El sistema mantiene varios estados:

```python
# Estados globales
- conversation_history: Lista de mensajes
- voice_enabled: Disponibilidad de voz
- use_stream: Modo streaming activo
- use_voice_output: TTS activo
- use_voice_streaming: TTS paralelo activo
```

### 2. Manejo de Errores

Estrategia de errores por capas:

```python
# Nivel 1: Verificaci√≥n de servicios
- Servidor Ollama disponible
- Modelo descargado
- M√≥dulos de voz importados

# Nivel 2: Errores de runtime
- Timeouts de red
- Errores de audio
- Fallos de s√≠ntesis

# Nivel 3: Recuperaci√≥n graceful
- Desactivar voz si falla
- Modo texto como fallback
- Mensajes informativos
```

### 3. Threading y Concurrencia

```python
# Hilos utilizados:
1. Main thread: UI y l√≥gica principal
2. TTS worker thread: S√≠ntesis en background
3. Audio callback thread: Captura de micr√≥fono
4. Pygame mixer thread: Reproducci√≥n de audio
```

### 4. Optimizaciones de Rendimiento

1. **Streaming HTTP**: Chunks de 8KB
2. **Audio buffering**: Queue thread-safe
3. **Cach√© de archivos**: Reutilizaci√≥n de MP3
4. **Lazy loading**: Inicializaci√≥n bajo demanda

## üêõ Resoluci√≥n de Problemas

### Problema: "El modelo Gemini no se pudo inicializar"
**Soluci√≥n:**
```bash
# Verificar que la API Key est√© configurada
echo $GOOGLE_API_KEY

# Si no est√° configurada, establecerla
export GOOGLE_API_KEY="tu-api-key-aqui"

# Luego ejecutar
python main.py
```

### Problema: "M√≥dulos de voz no disponibles"
**Soluci√≥n:**
```bash
pip install -r requirements.txt
# Verificar modelo Vosk en engine/voice/vosk-model-es-0.42
```

### Problema: "No se detect√≥ voz clara"
**Posibles causas:**
- Micr√≥fono no configurado
- Volumen muy bajo
- Ruido ambiental
- Timeout muy corto

### Problema: "Error en TTS"
**Verificar:**
- Conexi√≥n a internet (gTTS requiere internet)
- pygame correctamente instalado
- Permisos de audio del sistema

## üìù Notas Finales

Este sistema representa una integraci√≥n avanzada de:
- **IA Conversacional**: Mediante Google Gemini
- **Framework LangChain**: Para gesti√≥n profesional de LLMs
- **Tecnolog√≠as de Voz**: STT offline y TTS en streaming
- **Programaci√≥n As√≠ncrona**: Para experiencia fluida
- **Gesti√≥n de Contexto**: Con mensajes tipados de LangChain

El dise√±o modular permite extender f√°cilmente con:
- Otros modelos LLM mediante LangChain
- Cadenas de prompts complejas
- Memoria persistente
- Agentes y herramientas
- RAG (Retrieval Augmented Generation)
- Otros proveedores de IA (OpenAI, Anthropic, etc.)

# Aura - Asistente IA con Soporte MCP

Aura es un asistente de inteligencia artificial avanzado que utiliza **Google Gemini** a trav√©s de **LangChain** y soporta el **Model Context Protocol (MCP)** para conectarse con herramientas externas y fuentes de datos.

## üåü Caracter√≠sticas Principales

- **ü§ñ IA Avanzada**: Powered by Google Gemini 2.0 Flash Experimental
- **üé§ Reconocimiento de Voz**: Conversaciones naturales por voz
- **üîä S√≠ntesis de Voz**: Respuestas en audio con streaming
- **üîß Soporte MCP**: Conecta con herramientas externas (filesystem, APIs, etc.)
- **üí¨ Streaming**: Respuestas en tiempo real
- **üéØ Multimodal**: Soporte para texto, voz y herramientas

## üì¶ Instalaci√≥n

### Requisitos Previos

```bash
# Para Ubuntu/Debian
sudo apt update && sudo apt install python3 python3-pip nodejs npm portaudio19-dev

# Para macOS
brew install python node portaudio

# Para Windows
# Instalar Python desde python.org
# Instalar Node.js desde nodejs.org
# Instalar Visual Studio Build Tools
```

### Instalaci√≥n del Proyecto

```bash
# Clonar el repositorio
git clone <repository-url>
cd Aura_Ollama

# Crear entorno virtual
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Descargar modelo de voz (opcional)
# El modelo vosk-model-es-0.42 ya est√° incluido
```

## üîß Model Context Protocol (MCP)

### ¬øQu√© es MCP?

El **Model Context Protocol (MCP)** es un protocolo abierto desarrollado por Anthropic que estandariza c√≥mo las aplicaciones LLM se conectan con fuentes de datos externas y herramientas. Permite que tu asistente IA acceda a:

- **Sistema de archivos** (leer, escribir, buscar archivos)
- **APIs externas** (clima, noticias, bases de datos)
- **Herramientas especializadas** (calculadoras, convertidores)
- **Servicios web** (correo, calendarios, CRM)

### Configuraci√≥n de MCP

#### 1. Filesystem MCP (Incluido por defecto)

El servidor MCP de filesystem permite que Aura interact√∫e con tu sistema de archivos de manera segura:

```python
# Configuraci√≥n autom√°tica en main.py
mcp_config = {
    "filesystem": {
        "command": "npx",
        "args": [
            "-y",
            "@modelcontextprotocol/server-filesystem",
            "~/",              # Directorio home
            "~/Documents",     # Documentos
            "~/Desktop",       # Escritorio
            "~/Downloads"      # Descargas
        ],
        "transport": "stdio"
    }
}
```

#### 2. Herramientas Disponibles

Con el filesystem MCP, puedes pedirle a Aura que:

- **üìÅ Liste archivos**: "Mu√©strame los archivos en mi escritorio"
- **üìñ Lea archivos**: "Lee el contenido de mi archivo README.txt"
- **‚úèÔ∏è Cree archivos**: "Crea un archivo llamado 'notas.txt' con mis ideas"
- **üîç Busque archivos**: "Busca archivos que contengan 'proyecto' en el nombre"
- **üìä Info de archivos**: "Dame informaci√≥n sobre el archivo config.json"
- **üìÇ Cree directorios**: "Crea una carpeta llamada 'proyectos'"
- **üîÑ Mueva archivos**: "Mueve el archivo test.txt a la carpeta backup"

#### 3. Configuraci√≥n Personalizada

Puedes personalizar los servidores MCP editando el archivo `main.py`:

```python
# Configuraci√≥n personalizada
custom_mcp_config = {
    "filesystem": {
        "command": "npx", 
        "args": ["-y", "@modelcontextprotocol/server-filesystem", "/ruta/espec√≠fica"],
        "transport": "stdio"
    },
    "weather": {
        "command": "python",
        "args": ["path/to/weather_server.py"],
        "transport": "stdio"
    }
}
```

### Seguridad MCP

- **üîí Sandboxing**: MCPs solo acceden a directorios especificados
- **‚úÖ Validaci√≥n**: Rutas validadas para prevenir ataques de directorio
- **üõ°Ô∏è Permisos**: Control granular de qu√© puede hacer cada servidor
- **üîç Auditor√≠a**: Registro de todas las operaciones realizadas

## üöÄ Uso

### Modo B√°sico (Sin MCP)

```bash
python main.py
```

### Modo Con MCP (Recomendado)

```bash
# Aseg√∫rate de tener Node.js instalado
node --version  # Debe mostrar v14+ 

# Ejecutar Aura con MCP
python main.py
```

### Ejemplos de Comandos MCP

```bash
# Explorar archivos
"Lista los archivos en mi directorio Documents"
"¬øQu√© hay en mi escritorio?"

# Leer contenido
"Lee el archivo package.json y expl√≠came qu√© hace"
"Mu√©strame el contenido de mi archivo de configuraci√≥n"

# Crear y modificar
"Crea un archivo llamado 'ideas.md' con una lista de proyectos"
"Escribe un script Python b√°sico en mi escritorio"

# Buscar
"Busca todos los archivos .py en mi carpeta de proyectos"
"Encuentra archivos que contengan 'todo' en el nombre"

# Informaci√≥n
"Dame informaci√≥n detallada sobre el archivo m√°s grande en Downloads"
"¬øCu√°ndo fue modificado por √∫ltima vez mi archivo .bashrc?"
```

## üß™ Ejemplo de Prueba MCP

Usa el script de ejemplo incluido:

```bash
python example_mcp_usage.py
```

Este script te permite:
1. Probar el filesystem MCP interactivamente  
2. Ver configuraciones personalizadas
3. Aprender sobre las capacidades MCP

## üé§ Uso por Voz

```bash
# En la interfaz de chat, escribe:
voice

# Luego habla tu comando, por ejemplo:
"Lista mis archivos de escritorio"
"Crea un archivo con notas importantes"
```

## üèóÔ∏è Arquitectura T√©cnica

```mermaid
graph TB
    A[Usuario] --> B[Aura Main]
    B --> C[GeminiClient]
    C --> D[Google Gemini]
    C --> E[MCP Client]
    E --> F[Filesystem MCP]
    E --> G[Otros MCPs]
    F --> H[Sistema Archivos]
    C --> I[StreamingTTS]
    C --> J[VoiceRecognizer]
```

### Componentes MCP

1. **MultiServerMCPClient**: Gestiona conexiones a m√∫ltiples servidores MCP
2. **Tool Binding**: Integra herramientas MCP con el modelo LLM  
3. **Transport Layer**: Comunicaci√≥n stdio/HTTP con servidores
4. **Security Layer**: Validaci√≥n y sandboxing de operaciones

## üõ†Ô∏è Desarrollo y Extensi√≥n

### Agregar Nuevos Servidores MCP

1. **Instalar servidor MCP**:
```bash
npm install @modelcontextprotocol/server-newfeature
```

2. **Configurar en c√≥digo**:
```python
mcp_config = {
    "newfeature": {
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-newfeature"],
        "transport": "stdio"
    }
}
```

### Crear Servidor MCP Personalizado

```python
# Ejemplo: weather_server.py
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Weather")

@mcp.tool()
async def get_weather(location: str) -> str:
    """Obtener clima de una ubicaci√≥n"""
    # Tu l√≥gica aqu√≠
    return f"Clima en {location}: Soleado, 25¬∞C"

if __name__ == "__main__":
    mcp.run(transport="stdio")
```

## üîß Configuraci√≥n Avanzada

### Variables de Entorno

```bash
# .env file
GOOGLE_API_KEY=tu_api_key_aqui
MCP_FILESYSTEM_ROOT=/ruta/personalizada
MCP_ENABLE_LOGS=true
```

### Configuraci√≥n de Transporte

```python
# Configurar transporte HTTP en lugar de stdio
mcp_config = {
    "filesystem": {
        "url": "http://localhost:8000/mcp",
        "transport": "streamable_http"
    }
}
```

## üêõ Resoluci√≥n de Problemas MCP

### Error: "Node.js no encontrado"
```bash
# Instalar Node.js
curl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash -
sudo apt-get install -y nodejs
```

### Error: "MCP server no responde"
```bash
# Verificar que el servidor est√© disponible
npx @modelcontextprotocol/server-filesystem --version

# Verificar permisos de directorio
ls -la ~/Documents
```

### Error: "Tool not found"
```bash
# Reiniciar cliente MCP
# Las herramientas se cargan al inicio
```

## üìã Requisitos del Sistema

- **Python**: 3.8+
- **Node.js**: 14+
- **RAM**: 2GB m√≠nimo, 4GB recomendado
- **Espacio**: 1GB para modelos y dependencias
- **OS**: Linux, macOS, Windows 10+

## ü§ù Contribuir

Las contribuciones son bienvenidas, especialmente:

- **Nuevos servidores MCP**
- **Mejoras de seguridad** 
- **Optimizaciones de rendimiento**
- **Documentaci√≥n**
- **Tests**

## üìÑ Licencia

MIT License - ve el archivo LICENSE para m√°s detalles.

## üôè Agradecimientos

- **Anthropic** por el Model Context Protocol
- **Google** por Gemini API
- **LangChain** por la integraci√≥n
- **Comunidad MCP** por los servidores disponibles

---

### üîó Enlaces √ötiles

- [Documentaci√≥n MCP](https://modelcontextprotocol.io/)
- [Servidores MCP Disponibles](https://github.com/modelcontextprotocol/servers)
- [LangChain MCP Adapters](https://github.com/langchain-ai/langchain-mcp-adapters)
- [Google Gemini API](https://ai.google.dev/)

**¬°Disfruta usando Aura con el poder de MCP! üåü**

## üîç Integraci√≥n con Brave Search MCP

Aura ahora incluye integraci√≥n con **Model Context Protocol (MCP)** y espec√≠ficamente con **Brave Search**, permitiendo b√∫squedas web en tiempo real durante las conversaciones.

### Funcionalidades de Brave Search

- üåê **B√∫squedas web generales**: Noticias, art√≠culos, informaci√≥n actualizada
- üè™ **B√∫squedas locales**: Negocios, restaurantes, lugares cercanos
- üìä **Hasta 20 resultados** por b√∫squeda con soporte para paginaci√≥n
- üîÑ **Filtros de frescura** y contenido
- üöÄ **Integraci√≥n autom√°tica** con IA

### Configuraci√≥n MCP Disponible

Cuando ejecutes `python main.py`, tendr√°s estas opciones:

1. **üìÅ Solo Filesystem** - Operaciones con archivos locales
2. **üîç Solo Brave Search** - B√∫squedas web en tiempo real
3. **üåê Filesystem + Brave Search** - ¬°Recomendado! Ambas funcionalidades
4. **‚ùå Sin MCP** - Modo b√°sico sin herramientas adicionales

### Ejemplo de Uso

```bash
python main.py
# Selecciona modelo Gemini (opci√≥n 1)
# Habilita voz (s)  
# Selecciona "Filesystem + Brave Search" (opci√≥n 3)

# Ahora puedes hacer preguntas como:
üë§ T√∫: ¬øCu√°les son las √∫ltimas noticias sobre inteligencia artificial?
ü§ñ GEMINI: [Busca autom√°ticamente y responde con informaci√≥n actualizada]

üë§ T√∫: Busca restaurantes japoneses cerca de Madrid
ü§ñ GEMINI: [Utiliza b√∫squeda local para encontrar opciones]
```

### Modelos Recomendados

- **üü¢ Google Gemini** - Mejor para aprovechar las herramientas MCP
- **ü¶ô Ollama (qwen3:1.7b)** - Opci√≥n local m√°s ligera

**¬°Disfruta usando Aura con el poder de MCP! üåü** 