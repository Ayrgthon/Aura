# ğŸ¤– Aura_Gemini - Cliente Inteligente con Capacidades de Voz para Google Gemini

> âš¡ **IMPORTANTE**: Este sistema incluye streaming de texto en tiempo real y TTS (Text-to-Speech) en paralelo, permitiendo escuchar las respuestas mientras se generan. Esta funcionalidad es crÃ­tica para respuestas largas.

## ğŸ“‹ Ãndice
- [IntroducciÃ³n](#introducciÃ³n)
- [Arquitectura del Sistema](#arquitectura-del-sistema)
- [Requisitos y Dependencias](#requisitos-y-dependencias)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Algoritmo y Flujo de Trabajo](#algoritmo-y-flujo-de-trabajo)
- [Componentes Principales](#componentes-principales)
- [InstalaciÃ³n y ConfiguraciÃ³n](#instalaciÃ³n-y-configuraciÃ³n)
- [Uso del Sistema](#uso-del-sistema)
- [CaracterÃ­sticas Avanzadas](#caracterÃ­sticas-avanzadas)
- [Detalles TÃ©cnicos de ImplementaciÃ³n](#detalles-tÃ©cnicos-de-implementaciÃ³n)
- [ResoluciÃ³n de Problemas](#resoluciÃ³n-de-problemas)
- [IntegraciÃ³n con Brave Search MCP](#integraciÃ³n-con-brave-search-mcp)

## ğŸŒŸ IntroducciÃ³n

Aura_Gemini es un cliente inteligente para [Google Gemini](https://ai.google.dev/) usando [LangChain](https://langchain.com/) que extiende las capacidades del modelo de lenguaje con funcionalidades de voz bidireccionales. El sistema permite:

- **Entrada por voz**: Reconocimiento de voz en espaÃ±ol usando Vosk
- **Salida por voz**: SÃ­ntesis de voz con gTTS (Google Text-to-Speech)
- **Streaming en tiempo real**: Respuestas progresivas del modelo
- **TTS paralelo**: SÃ­ntesis de voz mientras el modelo genera respuestas
- **GestiÃ³n inteligente de contexto**: Mantiene historial de conversaciÃ³n con lÃ­mites configurables

## ğŸ—ï¸ Arquitectura del Sistema

```mermaid
graph TB
    User[Usuario] -->|Texto/Voz| Main[main.py]
    Main --> Client[GeminiClient]
    Client -->|LangChain| Gemini[Google Gemini API]
    Client --> VoiceEngine[Engine de Voz]
    VoiceEngine --> STT[hear.py<br/>Speech-to-Text]
    VoiceEngine --> TTS[speak.py<br/>Text-to-Speech]
    STT -->|Vosk| Mic[MicrÃ³fono]
    TTS -->|gTTS + pygame| Speaker[Altavoz]
    Gemini -->|Respuestas| Client
    Client -->|Streaming| User
```

### Componentes ArquitectÃ³nicos

1. **Capa de PresentaciÃ³n** (`main.py`)
   - Punto de entrada del sistema
   - GestiÃ³n de argumentos de lÃ­nea de comandos
   - VerificaciÃ³n de servicios disponibles

2. **Capa de LÃ³gica de Negocio** (`client.py`)
   - Cliente LangChain para Google Gemini API
   - GestiÃ³n de conversaciones y contexto con mensajes tipados
   - IntegraciÃ³n con mÃ³dulos de voz
   - Streaming de respuestas mediante generadores

3. **Capa de Servicios de Voz** (`engine/voice/`)
   - **STT (Speech-to-Text)**: Reconocimiento de voz offline
   - **TTS (Text-to-Speech)**: SÃ­ntesis de voz con streaming

## ğŸ“¦ Requisitos y Dependencias

### Dependencias de Python
```txt
requests>=2.31.0        # Cliente HTTP (usado para verificaciones)
langchain>=0.1.0        # Framework LangChain
langchain-google-genai>=0.0.6  # IntegraciÃ³n Google Gemini
google-generativeai>=0.3.0     # API de Google Generative AI
sounddevice>=0.4.6      # Captura de audio del micrÃ³fono
vosk>=0.3.45           # Motor de reconocimiento de voz offline
gtts>=2.4.0            # Google Text-to-Speech
pygame>=2.5.2          # ReproducciÃ³n de audio
numpy>=1.24.0          # Procesamiento de arrays (dependencia de vosk)
```

### Requisitos del Sistema
- **API Key de Google**: Clave de API para Google Generative AI
- **Modelo Vosk**: Modelo espaÃ±ol en `engine/voice/vosk-model-es-0.42`
- **Audio**: MicrÃ³fono y altavoces funcionales
- **Python**: 3.8 o superior
- **ConexiÃ³n a Internet**: Requerida para Google Gemini API

## ğŸ“ Estructura del Proyecto

```
Aura_Gemini/
â”œâ”€â”€ main.py                 # Punto de entrada principal
â”œâ”€â”€ client.py              # Cliente Gemini/LangChain con lÃ³gica de negocio
â”œâ”€â”€ requirements.txt       # Dependencias del proyecto
â””â”€â”€ engine/
    â””â”€â”€ voice/
        â”œâ”€â”€ hear.py        # MÃ³dulo STT (Speech-to-Text)
        â””â”€â”€ speak.py       # MÃ³dulo TTS (Text-to-Speech)
```

## ğŸ”„ Algoritmo y Flujo de Trabajo

### 1. InicializaciÃ³n del Sistema

```python
# main.py - Flujo de inicializaciÃ³n
def main():
    1. Configurar tamaÃ±o de contexto (100,000 tokens para Gemini)
    2. Verificar argumentos (--no-voice para desactivar voz)
    3. Crear instancia de GeminiClient
    4. Verificar disponibilidad del modelo
    5. Mostrar modelo activo
    6. Mostrar informaciÃ³n de contexto
    7. Decidir modo de operaciÃ³n:
       - Modo prompt Ãºnico: Si hay argumentos
       - Modo chat interactivo: Sin argumentos
```

### 2. Cliente Gemini - Algoritmo Principal

#### 2.1. InicializaciÃ³n del Cliente

```python
class GeminiClient:
    def __init__(self):
        - Configurar API Key de Google
        - Establecer modelo (gemini-2.0-flash-exp)
        - Inicializar modelo LangChain ChatGoogleGenerativeAI
        - Inicializar historial con BaseMessage de LangChain
        - Verificar disponibilidad de voz
        - Inicializar componentes de voz si estÃ¡n disponibles
```

#### 2.2. GeneraciÃ³n de Respuestas

El algoritmo de generaciÃ³n sigue estos pasos:

```python
def generate_response(prompt, stream, use_history, voice_streaming):
    1. Validar entrada no vacÃ­a y modelo inicializado
    2. Construir lista de mensajes (HumanMessage/AIMessage)
    3. Si use_history:
       - Incluir historial reciente (Ãºltimos 10 mensajes)
       - Mensajes ya tipados con LangChain
    4. Agregar mensaje actual como HumanMessage
    5. Si stream=True:
       - Usar generador stream() del modelo
       - Inicializar TTS paralelo si voice_streaming=True
       - Procesar respuesta chunk por chunk
       - Enviar cada chunk a TTS si estÃ¡ activo
    6. Si stream=False:
       - Usar invoke() para respuesta completa
       - Obtener content del response
    7. Actualizar historial con mensajes tipados
    8. Gestionar lÃ­mites de contexto automÃ¡ticamente
```

#### 2.3. GestiÃ³n de Contexto

```python
def _trim_history_if_needed():
    # Algoritmo de recorte inteligente
    1. Calcular tokens totales (1 token â‰ˆ 4 caracteres)
    2. Si tokens > 75% del contexto mÃ¡ximo:
       - Eliminar mensajes mÃ¡s antiguos
       - Mantener al menos 1 par usuario-asistente
    3. Preservar coherencia conversacional
```

### 3. Sistema de Voz - Algoritmos

#### 3.1. Speech-to-Text (STT) - hear.py

```python
# Algoritmo de reconocimiento de voz
def listen_for_command(recognizer, timeout=5):
    1. Limpiar cola de audio previa
    2. Abrir stream de audio:
       - Sample rate: 16000 Hz
       - Blocksize: 8000
       - Mono, int16
    3. Capturar audio en tiempo real:
       - Callback envÃ­a datos a cola
       - Procesar chunks con Vosk
    4. Detectar fin de habla:
       - Si AcceptWaveform() = True: frase completa
       - Si timeout: terminar captura
    5. Obtener resultado final con FinalResult()
    6. Devolver texto reconocido
```

#### 3.2. Text-to-Speech (TTS) - speak.py

```python
# Algoritmo de sÃ­ntesis bÃ¡sica
def speak(text):
    1. Validar texto no vacÃ­o
    2. Generar audio con gTTS:
       - Idioma: espaÃ±ol
       - Velocidad: normal/lenta
    3. Guardar en archivo temporal MP3
    4. Cargar con pygame.mixer
    5. Reproducir y esperar finalizaciÃ³n
    6. Limpiar archivo temporal
```

#### 3.3. TTS Streaming Secuencial

```python
class StreamingTTS:
    # Algoritmo de TTS en streaming con orden garantizado
    def _tts_worker():
        1. Mantener buffer de texto y contador de palabras
        2. Por cada chunk recibido:
           - Agregar a buffer
           - Buscar finales de oraciÃ³n (. ! ? \n : ;)
        3. Si encuentra oraciÃ³n completa:
           - Extraer oraciÃ³n del buffer
           - Sintetizar y reproducir secuencialmente (con lock)
        4. Si buffer > 8 palabras sin puntuaciÃ³n:
           - Tomar primeras 6 palabras
           - Reproducir y mantener resto en buffer
        5. Usar threading.Lock para garantizar orden secuencial
        6. Al recibir seÃ±al de fin:
           - Procesar texto restante
           - Terminar thread
```

### 4. Flujo de Chat Interactivo

```python
def chat():
    1. Mostrar informaciÃ³n inicial
    2. Loop principal:
       a. Leer entrada del usuario
       b. Procesar comandos especiales:
          - 'salir/exit': Terminar
          - 'stream': Toggle streaming
          - 'historial': Mostrar conversaciÃ³n
          - 'limpiar': Borrar historial
          - 'info': EstadÃ­sticas de contexto
          - 'escuchar': Activar STT
          - 'voz': Toggle TTS
          - 'voz-streaming': TTS paralelo
       c. Si es pregunta normal:
          - Generar respuesta con configuraciÃ³n actual
          - Si streaming + voz: TTS paralelo
          - Si no streaming + voz: TTS despuÃ©s
       d. Manejar interrupciones (Ctrl+C)
```

## ğŸ§© Componentes Principales

### 1. GeminiClient (client.py)

**Responsabilidades:**
- IntegraciÃ³n con Google Gemini mediante LangChain
- GestiÃ³n de contexto con mensajes tipados (HumanMessage/AIMessage)
- IntegraciÃ³n de capacidades de voz
- Streaming de respuestas mediante generadores

**MÃ©todos Principales:**
- `generate_response()`: NÃºcleo de generaciÃ³n con LangChain
- `_stream_response()`: Procesamiento con generador stream()
- `_get_recent_history()`: ObtenciÃ³n de historial reciente
- `chat()`: Loop interactivo principal

### 2. MÃ³dulo STT (engine/voice/hear.py)

**CaracterÃ­sticas:**
- Reconocimiento offline con Vosk
- Modelo espaÃ±ol preentrenado
- Procesamiento en tiempo real
- DetecciÃ³n automÃ¡tica de fin de habla

**ConfiguraciÃ³n:**
- Sample rate: 16 kHz (Ã³ptimo para voz)
- Blocksize: 8000 (latencia vs. calidad)
- Timeout configurable

### 3. MÃ³dulo TTS (engine/voice/speak.py)

**CaracterÃ­sticas:**
- SÃ­ntesis con gTTS (requiere internet)
- ReproducciÃ³n con pygame
- Modo sÃ­ncrono y asÃ­ncrono
- Streaming paralelo inteligente

**Clases:**
- `VoiceSynthesizer`: SÃ­ntesis bÃ¡sica
- `StreamingTTS`: TTS paralelo con buffer inteligente

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio
```bash
git clone https://github.com/tuusuario/Aura_Gemini.git
cd Aura_Gemini
```

### 2. Instalar Dependencias
```bash
pip install -r requirements.txt
```

### 3. Descargar Modelo Vosk
```bash
# Crear directorio
mkdir -p engine/voice

# Descargar modelo espaÃ±ol
cd engine/voice
wget https://alphacephei.com/vosk/models/vosk-model-es-0.42.zip
unzip vosk-model-es-0.42.zip
rm vosk-model-es-0.42.zip
cd ../..
```

### 4. Configurar API Key de Google
```bash
# OpciÃ³n 1: Configurar en variable de entorno
export GOOGLE_API_KEY="tu-api-key-aqui"

# OpciÃ³n 2: La API key ya estÃ¡ incluida en el cÃ³digo
# pero se recomienda usar tu propia clave
```

Para obtener una API Key:
1. Ve a [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Crea o selecciona un proyecto
3. Genera una nueva API Key
4. Copia y guarda la clave de forma segura

## ğŸ’» Uso del Sistema

### Modo Chat Interactivo
```bash
python main.py
```

### Modo Prompt Ãšnico
```bash
python main.py "Â¿CuÃ¡l es la capital de EspaÃ±a?"
```

### Desactivar Voz
```bash
python main.py --no-voice
```

### Comandos del Chat

| Comando | DescripciÃ³n |
|---------|-------------|
| `salir` / `exit` | Terminar sesiÃ³n |
| `stream` | Activar/desactivar streaming |
| `historial` | Ver conversaciÃ³n completa |
| `limpiar` | Borrar historial |
| `info` | Ver uso de contexto |
| `escuchar` | Entrada por voz |
| `voz` | Activar/desactivar TTS |
| `voz-streaming` | TTS paralelo (recomendado) |

## ğŸ”§ CaracterÃ­sticas Avanzadas

### 1. GestiÃ³n Inteligente de Contexto

El sistema mantiene un historial de conversaciÃ³n optimizado:

- **LÃ­mite de tokens**: 100,000 para Gemini Flash
- **EstimaciÃ³n**: 1 token â‰ˆ 4 caracteres
- **Mensajes tipados**: HumanMessage y AIMessage de LangChain
- **Recorte automÃ¡tico**: Mantiene coherencia eliminando mensajes antiguos
- **PreservaciÃ³n**: Siempre mantiene al menos un par pregunta-respuesta

### 2. Streaming en Tiempo Real

```python
# Ventajas del streaming:
- Latencia reducida: Primera respuesta en <1s
- Experiencia fluida: Ver respuesta mientras se genera
- TTS paralelo: Escuchar mientras se genera
- InterrupciÃ³n: Posibilidad de cancelar
```

### 3. TTS Paralelo Inteligente

El algoritmo de TTS secuencial:

1. **DetecciÃ³n de oraciones**: Busca puntuaciÃ³n final (. ! ? : ; \n)
2. **Buffer inteligente**: Acumula texto y cuenta palabras
3. **Corte por palabras**: Si >8 palabras, reproduce primeras 6
4. **Orden garantizado**: Threading.Lock evita reproducciÃ³n paralela
5. **MÃ©todo secuencial**: `speak_chunk_sequential()` vs `speak_chunk_async()`
6. **Sin solapamiento**: Cada fragmento espera al anterior

### ğŸ”§ SoluciÃ³n al Problema de Orden

**Problema anterior**: Los chunks se reproducÃ­an en paralelo causando desorden
**SoluciÃ³n implementada**:
- Lock de threading para serializar reproducciÃ³n
- MÃ©todo `speak_chunk_sequential()` que bloquea hasta terminar
- Buffer que agrupa por palabras completas, no caracteres
- Prioridad a oraciones completas antes que fragmentos

### 4. ConfiguraciÃ³n del Modelo

```python
ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    max_output_tokens=2048,    # Tokens mÃ¡ximos por respuesta
    temperature=0.7,           # Creatividad (0-1)
    top_p=0.9,                # Nucleus sampling
    top_k=40                  # Top-k sampling
)
```

## ğŸ” Detalles TÃ©cnicos de ImplementaciÃ³n

### 1. Manejo de Estado

El sistema mantiene varios estados:

```python
# Estados globales
- conversation_history: Lista de mensajes
- voice_enabled: Disponibilidad de voz
- use_stream: Modo streaming activo
- use_voice_output: TTS activo
- use_voice_streaming: TTS paralelo activo
```

### 2. Manejo de Errores

Estrategia de errores por capas:

```python
# Nivel 1: VerificaciÃ³n de servicios
- Servidor Ollama disponible
- Modelo descargado
- MÃ³dulos de voz importados

# Nivel 2: Errores de runtime
- Timeouts de red
- Errores de audio
- Fallos de sÃ­ntesis

# Nivel 3: RecuperaciÃ³n graceful
- Desactivar voz si falla
- Modo texto como fallback
- Mensajes informativos
```

### 3. Threading y Concurrencia

```python
# Hilos utilizados:
1. Main thread: UI y lÃ³gica principal
2. TTS worker thread: SÃ­ntesis en background
3. Audio callback thread: Captura de micrÃ³fono
4. Pygame mixer thread: ReproducciÃ³n de audio
```

### 4. Optimizaciones de Rendimiento

1. **Streaming HTTP**: Chunks de 8KB
2. **Audio buffering**: Queue thread-safe
3. **CachÃ© de archivos**: ReutilizaciÃ³n de MP3
4. **Lazy loading**: InicializaciÃ³n bajo demanda

## ğŸ› ResoluciÃ³n de Problemas

### Problema: "El modelo Gemini no se pudo inicializar"
**SoluciÃ³n:**
```bash
# Verificar que la API Key estÃ© configurada
echo $GOOGLE_API_KEY

# Si no estÃ¡ configurada, establecerla
export GOOGLE_API_KEY="tu-api-key-aqui"

# Luego ejecutar
python main.py
```

### Problema: "MÃ³dulos de voz no disponibles"
**SoluciÃ³n:**
```bash
pip install -r requirements.txt
# Verificar modelo Vosk en engine/voice/vosk-model-es-0.42
```

### Problema: "No se detectÃ³ voz clara"
**Posibles causas:**
- MicrÃ³fono no configurado
- Volumen muy bajo
- Ruido ambiental
- Timeout muy corto

### Problema: "Error en TTS"
**Verificar:**
- ConexiÃ³n a internet (gTTS requiere internet)
- pygame correctamente instalado
- Permisos de audio del sistema

## ğŸ“ Notas Finales

Este sistema representa una integraciÃ³n avanzada de:
- **IA Conversacional**: Mediante Google Gemini
- **Framework LangChain**: Para gestiÃ³n profesional de LLMs
- **TecnologÃ­as de Voz**: STT offline y TTS en streaming
- **ProgramaciÃ³n AsÃ­ncrona**: Para experiencia fluida
- **GestiÃ³n de Contexto**: Con mensajes tipados de LangChain

El diseÃ±o modular permite extender fÃ¡cilmente con:
- Otros modelos LLM mediante LangChain
- Cadenas de prompts complejas
- Memoria persistente
- Agentes y herramientas
- RAG (Retrieval Augmented Generation)
- Otros proveedores de IA (OpenAI, Anthropic, etc.)

# Aura - Asistente IA con Soporte MCP

Aura es un asistente de inteligencia artificial avanzado que utiliza **Google Gemini** a travÃ©s de **LangChain** y soporta el **Model Context Protocol (MCP)** para conectarse con herramientas externas y fuentes de datos.

## ğŸŒŸ CaracterÃ­sticas Principales

- **ğŸ¤– IA Avanzada**: Powered by Google Gemini 2.0 Flash Experimental
- **ğŸ¤ Reconocimiento de Voz**: Conversaciones naturales por voz
- **ğŸ”Š SÃ­ntesis de Voz**: Respuestas en audio con streaming
- **ğŸ”§ Soporte MCP**: Conecta con herramientas externas (filesystem, APIs, etc.)
- **ğŸ’¬ Streaming**: Respuestas en tiempo real
- **ğŸ¯ Multimodal**: Soporte para texto, voz y herramientas

## ğŸ“¦ InstalaciÃ³n

### Requisitos Previos

```bash
# Para Ubuntu/Debian
sudo apt update && sudo apt install python3 python3-pip nodejs npm portaudio19-dev

# Para macOS
brew install python node portaudio

# Para Windows
# Instalar Python desde python.org
# Instalar Node.js desde nodejs.org
# Instalar Visual Studio Build Tools
```

### InstalaciÃ³n del Proyecto

```bash
# Clonar el repositorio
git clone <repository-url>
cd Aura_Ollama

# Crear entorno virtual
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Descargar modelo de voz (opcional)
# El modelo vosk-model-es-0.42 ya estÃ¡ incluido
```

## ğŸ”§ Model Context Protocol (MCP)

### Â¿QuÃ© es MCP?

El **Model Context Protocol (MCP)** es un protocolo abierto desarrollado por Anthropic que estandariza cÃ³mo las aplicaciones LLM se conectan con fuentes de datos externas y herramientas. Permite que tu asistente IA acceda a:

- **Sistema de archivos** (leer, escribir, buscar archivos)
- **APIs externas** (clima, noticias, bases de datos)
- **Herramientas especializadas** (calculadoras, convertidores)
- **Servicios web** (correo, calendarios, CRM)

### ConfiguraciÃ³n de MCP

#### 1. Filesystem MCP (Incluido por defecto)

El servidor MCP de filesystem permite que Aura interactÃºe con tu sistema de archivos de manera segura:

```python
# ConfiguraciÃ³n automÃ¡tica en main.py
mcp_config = {
    "filesystem": {
        "command": "npx",
        "args": [
            "-y",
            "@modelcontextprotocol/server-filesystem",
            "~/",              # Directorio home
            "~/Documents",     # Documentos
            "~/Desktop",       # Escritorio
            "~/Downloads"      # Descargas
        ],
        "transport": "stdio"
    }
}
```

#### 2. Herramientas Disponibles

Con el filesystem MCP, puedes pedirle a Aura que:

- **ğŸ“ Liste archivos**: "MuÃ©strame los archivos en mi escritorio"
- **ğŸ“– Lea archivos**: "Lee el contenido de mi archivo README.txt"
- **âœï¸ Cree archivos**: "Crea un archivo llamado 'notas.txt' con mis ideas"
- **ğŸ” Busque archivos**: "Busca archivos que contengan 'proyecto' en el nombre"
- **ğŸ“Š Info de archivos**: "Dame informaciÃ³n sobre el archivo config.json"
- **ğŸ“‚ Cree directorios**: "Crea una carpeta llamada 'proyectos'"
- **ğŸ”„ Mueva archivos**: "Mueve el archivo test.txt a la carpeta backup"

#### 3. ConfiguraciÃ³n Personalizada

Puedes personalizar los servidores MCP editando el archivo `main.py`:

```python
# ConfiguraciÃ³n personalizada
custom_mcp_config = {
    "filesystem": {
        "command": "npx", 
        "args": ["-y", "@modelcontextprotocol/server-filesystem", "/ruta/especÃ­fica"],
        "transport": "stdio"
    },
    "weather": {
        "command": "python",
        "args": ["path/to/weather_server.py"],
        "transport": "stdio"
    }
}
```

### Seguridad MCP

- **ğŸ”’ Sandboxing**: MCPs solo acceden a directorios especificados
- **âœ… ValidaciÃ³n**: Rutas validadas para prevenir ataques de directorio
- **ğŸ›¡ï¸ Permisos**: Control granular de quÃ© puede hacer cada servidor
- **ğŸ” AuditorÃ­a**: Registro de todas las operaciones realizadas

## ğŸš€ Uso

### Modo BÃ¡sico (Sin MCP)

```bash
python main.py
```

### Modo Con MCP (Recomendado)

```bash
# AsegÃºrate de tener Node.js instalado
node --version  # Debe mostrar v14+ 

# Ejecutar Aura con MCP
python main.py
```

### Ejemplos de Comandos MCP

```bash
# Explorar archivos
"Lista los archivos en mi directorio Documents"
"Â¿QuÃ© hay en mi escritorio?"

# Leer contenido
"Lee el archivo package.json y explÃ­came quÃ© hace"
"MuÃ©strame el contenido de mi archivo de configuraciÃ³n"

# Crear y modificar
"Crea un archivo llamado 'ideas.md' con una lista de proyectos"
"Escribe un script Python bÃ¡sico en mi escritorio"

# Buscar
"Busca todos los archivos .py en mi carpeta de proyectos"
"Encuentra archivos que contengan 'todo' en el nombre"

# InformaciÃ³n
"Dame informaciÃ³n detallada sobre el archivo mÃ¡s grande en Downloads"
"Â¿CuÃ¡ndo fue modificado por Ãºltima vez mi archivo .bashrc?"
```

## ğŸ§ª Ejemplo de Prueba MCP

Usa el script de ejemplo incluido:

```bash
python example_mcp_usage.py
```

Este script te permite:
1. Probar el filesystem MCP interactivamente  
2. Ver configuraciones personalizadas
3. Aprender sobre las capacidades MCP

## ğŸ¤ Uso por Voz

```bash
# En la interfaz de chat, escribe:
voice

# Luego habla tu comando, por ejemplo:
"Lista mis archivos de escritorio"
"Crea un archivo con notas importantes"
```

## ğŸ—ï¸ Arquitectura TÃ©cnica

```mermaid
graph TB
    A[Usuario] --> B[Aura Main]
    B --> C[GeminiClient]
    C --> D[Google Gemini]
    C --> E[MCP Client]
    E --> F[Filesystem MCP]
    E --> G[Otros MCPs]
    F --> H[Sistema Archivos]
    C --> I[StreamingTTS]
    C --> J[VoiceRecognizer]
```

### Componentes MCP

1. **MultiServerMCPClient**: Gestiona conexiones a mÃºltiples servidores MCP
2. **Tool Binding**: Integra herramientas MCP con el modelo LLM  
3. **Transport Layer**: ComunicaciÃ³n stdio/HTTP con servidores
4. **Security Layer**: ValidaciÃ³n y sandboxing de operaciones

## ğŸ› ï¸ Desarrollo y ExtensiÃ³n

### Agregar Nuevos Servidores MCP

1. **Instalar servidor MCP**:
```bash
npm install @modelcontextprotocol/server-newfeature
```

2. **Configurar en cÃ³digo**:
```python
mcp_config = {
    "newfeature": {
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-newfeature"],
        "transport": "stdio"
    }
}
```

### Crear Servidor MCP Personalizado

```python
# Ejemplo: weather_server.py
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Weather")

@mcp.tool()
async def get_weather(location: str) -> str:
    """Obtener clima de una ubicaciÃ³n"""
    # Tu lÃ³gica aquÃ­
    return f"Clima en {location}: Soleado, 25Â°C"

if __name__ == "__main__":
    mcp.run(transport="stdio")
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno

```bash
# .env file
GOOGLE_API_KEY=tu_api_key_aqui
MCP_FILESYSTEM_ROOT=/ruta/personalizada
MCP_ENABLE_LOGS=true
```

### ConfiguraciÃ³n de Transporte

```python
# Configurar transporte HTTP en lugar de stdio
mcp_config = {
    "filesystem": {
        "url": "http://localhost:8000/mcp",
        "transport": "streamable_http"
    }
}
```

## ğŸ› ResoluciÃ³n de Problemas MCP

### Error: "Node.js no encontrado"
```bash
# Instalar Node.js
curl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash -
sudo apt-get install -y nodejs
```

### Error: "MCP server no responde"
```bash
# Verificar que el servidor estÃ© disponible
npx @modelcontextprotocol/server-filesystem --version

# Verificar permisos de directorio
ls -la ~/Documents
```

### Error: "Tool not found"
```bash
# Reiniciar cliente MCP
# Las herramientas se cargan al inicio
```

## ğŸ“‹ Requisitos del Sistema

- **Python**: 3.8+
- **Node.js**: 14+
- **RAM**: 2GB mÃ­nimo, 4GB recomendado
- **Espacio**: 1GB para modelos y dependencias
- **OS**: Linux, macOS, Windows 10+

## ğŸ¤ Contribuir

Las contribuciones son bienvenidas, especialmente:

- **Nuevos servidores MCP**
- **Mejoras de seguridad** 
- **Optimizaciones de rendimiento**
- **DocumentaciÃ³n**
- **Tests**

## ğŸ“„ Licencia

MIT License - ve el archivo LICENSE para mÃ¡s detalles.

## ğŸ™ Agradecimientos

- **Anthropic** por el Model Context Protocol
- **Google** por Gemini API
- **LangChain** por la integraciÃ³n
- **Comunidad MCP** por los servidores disponibles

---

### ğŸ”— Enlaces Ãštiles

- [DocumentaciÃ³n MCP](https://modelcontextprotocol.io/)
- [Servidores MCP Disponibles](https://github.com/modelcontextprotocol/servers)
- [LangChain MCP Adapters](https://github.com/langchain-ai/langchain-mcp-adapters)
- [Google Gemini API](https://ai.google.dev/)

**Â¡Disfruta usando Aura con el poder de MCP! ğŸŒŸ**

## ğŸ” IntegraciÃ³n con Brave Search MCP

Aura ahora incluye integraciÃ³n con **Model Context Protocol (MCP)** y especÃ­ficamente con **Brave Search**, permitiendo bÃºsquedas web en tiempo real durante las conversaciones.

### Funcionalidades de Brave Search

- ğŸŒ **BÃºsquedas web generales**: Noticias, artÃ­culos, informaciÃ³n actualizada
- ğŸª **BÃºsquedas locales**: Negocios, restaurantes, lugares cercanos
- ğŸ“Š **Hasta 20 resultados** por bÃºsqueda con soporte para paginaciÃ³n
- ğŸ”„ **Filtros de frescura** y contenido
- ğŸš€ **IntegraciÃ³n automÃ¡tica** con IA

### ConfiguraciÃ³n MCP Disponible

Cuando ejecutes `python main.py`, tendrÃ¡s estas opciones:

1. **ğŸ“ Solo Filesystem** - Operaciones con archivos locales
2. **ğŸ” Solo Brave Search** - BÃºsquedas web en tiempo real
3. **ğŸŒ Filesystem + Brave Search** - Â¡Recomendado! Ambas funcionalidades
4. **âŒ Sin MCP** - Modo bÃ¡sico sin herramientas adicionales

### Ejemplo de Uso

```bash
python main.py
# Selecciona modelo Gemini (opciÃ³n 1)
# Habilita voz (s)  
# Selecciona "Filesystem + Brave Search" (opciÃ³n 3)

# Ahora puedes hacer preguntas como:
ğŸ‘¤ TÃº: Â¿CuÃ¡les son las Ãºltimas noticias sobre inteligencia artificial?
ğŸ¤– GEMINI: [Busca automÃ¡ticamente y responde con informaciÃ³n actualizada]

ğŸ‘¤ TÃº: Busca restaurantes japoneses cerca de Madrid
ğŸ¤– GEMINI: [Utiliza bÃºsqueda local para encontrar opciones]
```

### Modelos Recomendados

- **ğŸŸ¢ Google Gemini** - Mejor para aprovechar las herramientas MCP
- **ğŸ¦™ Ollama (qwen3:1.7b)** - OpciÃ³n local mÃ¡s ligera

**Â¡Disfruta usando Aura con el poder de MCP! ğŸŒŸ** 