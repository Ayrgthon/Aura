#!/usr/bin/env python3
"""
Punto de entrada principal para AuraOllama
Cliente de Ollama con funcionalidades de voz integradas
"""

import sys
from client import OllamaClient

def main():
    """
    FunciÃ³n principal del script
    """
    # Permitir configurar el contexto desde argumentos
    context_size = 130000  # MÃ¡ximo para gemma3:4b
    
    # Verificar si se debe deshabilitar la voz
    disable_voice = '--no-voice' in sys.argv
    if disable_voice:
        sys.argv.remove('--no-voice')
    
    client = OllamaClient(context_size=context_size, enable_voice=not disable_voice)
    
    # Verificar si el servidor estÃ¡ ejecutÃ¡ndose
    if not client.is_server_running():
        print("âŒ Error: El servidor de Ollama no estÃ¡ ejecutÃ¡ndose")
        print("ðŸ’¡ AsegÃºrate de que Ollama estÃ© iniciado con: ollama serve")
        sys.exit(1)
    
    print("âœ… Conectado al servidor de Ollama")
    
    # Listar modelos disponibles
    models = client.list_models()
    if models:
        print(f"ðŸ“‹ Modelos disponibles: {', '.join(models)}")
    
    # Verificar si el modelo estÃ¡ disponible
    if client.model not in models:
        print(f"âš ï¸  Advertencia: El modelo '{client.model}' no se encuentra en la lista")
        print(f"ðŸ’¡ Puedes descargarlo con: ollama pull {client.model}")
    
    # Mostrar informaciÃ³n de contexto
    client.show_context_info()
    
    # Modo de uso
    if len(sys.argv) > 1:
        # Modo prompt Ãºnico con streaming por defecto
        prompt = " ".join(sys.argv[1:])
        print(f"ðŸ¤– Respuesta para: '{prompt}'")
        print("ðŸŽ¬ Streaming activado:")
        client.generate_response(prompt, stream=True)
    else:
        # Modo chat interactivo
        client.chat()

if __name__ == "__main__":
    main() 