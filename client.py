#!/usr/bin/env python3
import requests
import json
import sys
import os

# Importar m√≥dulos de voz
try:
    from engine.voice.hear import initialize_recognizer, listen_for_command
    from engine.voice.speak import (
        speak, stop_speaking, is_speaking,
        start_streaming_tts, add_text_to_stream, finish_streaming_tts
    )
    VOICE_AVAILABLE = True
    print("‚úÖ M√≥dulos de voz cargados correctamente")
    print("üîß Funciones streaming TTS importadas:", 
          start_streaming_tts.__name__, add_text_to_stream.__name__, finish_streaming_tts.__name__)
except ImportError as e:
    VOICE_AVAILABLE = False
    print(f"‚ö†Ô∏è M√≥dulos de voz no disponibles: {e}")
    print("üí° Instala las dependencias con: pip install -r requirements.txt")

class OllamaClient:
    def __init__(self, host="localhost", port=11434, context_size=100000, enable_voice=True):
        """
        Inicializa el cliente de Ollama
        
        Args:
            host (str): Direcci√≥n del servidor de Ollama (por defecto localhost)
            port (int): Puerto del servidor de Ollama (por defecto 11434)
            context_size (int): Tama√±o del contexto en tokens (por defecto 130000 para gemma3:4b)
            enable_voice (bool): Si True, habilita funcionalidades de voz
        """
        self.base_url = f"http://{host}:{port}"
        self.model = "gemma3:4b"
        self.context_size = context_size
        self.conversation_history = []  # Para mantener historial como en terminal
        
        # Configuraci√≥n de voz
        self.voice_enabled = enable_voice and VOICE_AVAILABLE
        self.voice_recognizer = None
        
        if self.voice_enabled:
            self._initialize_voice()
    
    def _initialize_voice(self):
        """
        Inicializa los componentes de voz
        """
        try:
            self.voice_recognizer = initialize_recognizer()
            if self.voice_recognizer:
                print("üé§ Sistema de voz activado")
            else:
                print("‚ùå Error al inicializar reconocimiento de voz")
                self.voice_enabled = False
        except Exception as e:
            print(f"‚ùå Error al inicializar voz: {e}")
            self.voice_enabled = False
    
    def listen_to_voice(self, timeout=5):
        """
        Escucha entrada de voz del usuario
        
        Args:
            timeout (int): Tiempo l√≠mite de escucha
            
        Returns:
            str: Texto reconocido de la voz
        """
        if not self.voice_enabled or not self.voice_recognizer:
            print("‚ùå Funcionalidad de voz no disponible")
            return ""
        
        return listen_for_command(self.voice_recognizer, timeout)
    
    def speak_response(self, text):
        """
        Convierte respuesta a voz
        
        Args:
            text (str): Texto a convertir en voz
        """
        if not self.voice_enabled:
            return
        
        speak(text)

    def is_server_running(self):
        """
        Verifica si el servidor de Ollama est√° ejecut√°ndose
        
        Returns:
            bool: True si el servidor est√° activo, False en caso contrario
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False
    
    def list_models(self):
        """
        Lista todos los modelos disponibles en Ollama
        
        Returns:
            list: Lista de modelos disponibles
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
            else:
                print(f"Error al obtener modelos: {response.status_code}")
                return []
        except requests.exceptions.RequestException as e:
            print(f"Error de conexi√≥n: {e}")
            return []
    
    def generate_response(self, prompt, stream=False, use_history=True, voice_streaming=False):
        """
        Genera una respuesta usando el modelo especificado
        
        Args:
            prompt (str): El prompt/pregunta para el modelo
            stream (bool): Si True, transmite la respuesta en tiempo real
            use_history (bool): Si True, incluye el historial de conversaci√≥n
            voice_streaming (bool): Si True, activa TTS en paralelo
            
        Returns:
            str: La respuesta del modelo
        """
        if not prompt.strip():
            return "Prompt vac√≠o"
            
        url = f"{self.base_url}/api/generate"
        
        # Construir el prompt con historial si est√° habilitado
        if use_history and self.conversation_history:
            full_prompt = self._build_prompt_with_history(prompt)
        else:
            full_prompt = prompt
        
        # Configuraci√≥n del modelo
        payload = {
            "model": self.model,
            "prompt": full_prompt,
            "stream": stream,
            "options": {
                "num_ctx": self.context_size,
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "num_predict": 128,
                "stop": ["Usuario:", "T√∫:", "Human:", "Assistant:"],
                "repeat_penalty": 1.1
            }
        }
        
        try:
            if stream:
                response_text = self._stream_response(url, payload, voice_streaming)
            else:
                response = requests.post(url, json=payload)
                if response.status_code != 200:
                    error_msg = f"Error: {response.status_code} - {response.text}"
                    print(error_msg)
                    return error_msg
                    
                try:
                    data = response.json()
                except json.JSONDecodeError as e:
                    error_msg = f"Error decodificando respuesta: {e}"
                    print(error_msg)
                    return error_msg
                    
                if 'error' in data:
                    error_msg = f"Error del modelo: {data['error']}"
                    print(error_msg)
                    return error_msg
                    
                response_text = data.get('response', '')
                if not response_text.strip():
                    print("‚ö†Ô∏è La respuesta del modelo est√° vac√≠a")
                    return "Sin respuesta"
            
            # Agregar al historial si est√° habilitado y hay respuesta v√°lida
            if use_history and response_text and response_text != "Sin respuesta":
                self._add_to_history("user", prompt)
                self._add_to_history("assistant", response_text)
            
            return response_text
            
        except requests.exceptions.RequestException as e:
            error_msg = f"Error de conexi√≥n: {e}"
            print(error_msg)
            return error_msg
    
    def _build_prompt_with_history(self, current_prompt):
        """
        Construye un prompt que incluye el historial de conversaci√≥n
        
        Args:
            current_prompt (str): El prompt actual del usuario
            
        Returns:
            str: Prompt completo con historial
        """
        # Limitar el n√∫mero de mensajes en el historial para evitar sobrecargar el contexto
        max_history_messages = 10  # Ajustar seg√∫n necesidad
        recent_history = self.conversation_history[-max_history_messages:] if len(self.conversation_history) > max_history_messages else self.conversation_history
        
        history_text = "A continuaci√≥n hay una conversaci√≥n entre un humano y un asistente de IA.\n\n"
        
        for entry in recent_history:
            if entry['role'] == 'user':
                history_text += f"Humano: {entry['content']}\n\n"
            else:
                history_text += f"Asistente: {entry['content']}\n\n"
        
        # Agregar el prompt actual
        history_text += f"Humano: {current_prompt}\n\nAsistente:"
        
        return history_text
    
    def _add_to_history(self, role, content):
        """
        Agrega una entrada al historial de conversaci√≥n
        
        Args:
            role (str): 'user' o 'assistant'
            content (str): El contenido del mensaje
        """
        self.conversation_history.append({
            "role": role,
            "content": content
        })
        
        # Mantener el historial dentro del l√≠mite de contexto
        self._trim_history_if_needed()
    
    def _trim_history_if_needed(self):
        """
        Recorta el historial si excede el l√≠mite de contexto
        """
        # Estimaci√≥n aproximada: 1 token ‚âà 4 caracteres
        total_chars = sum(len(entry['content']) for entry in self.conversation_history)
        max_chars = self.context_size * 3  # Dejar espacio para la respuesta
        
        while total_chars > max_chars and len(self.conversation_history) > 2:
            # Remover las entradas m√°s antiguas (pero mantener al menos 1 par)
            removed = self.conversation_history.pop(0)
            total_chars -= len(removed['content'])
    
    def clear_history(self):
        """
        Limpia el historial de conversaci√≥n
        """
        self.conversation_history = []
        print("üóëÔ∏è Historial de conversaci√≥n limpiado")
    
    def show_context_info(self):
        """
        Muestra informaci√≥n sobre el contexto configurado
        """
        print(f"üìä Informaci√≥n de Contexto:")
        print(f"   ‚Ä¢ Modelo: {self.model}")
        print(f"   ‚Ä¢ Contexto configurado: {self.context_size:,} tokens")
        print(f"   ‚Ä¢ Historial: {len(self.conversation_history)} mensajes")
        
        if self.conversation_history:
            total_chars = sum(len(entry['content']) for entry in self.conversation_history)
            estimated_tokens = total_chars // 4
            print(f"   ‚Ä¢ Tokens estimados en historial: {estimated_tokens:,}")
            usage_percent = (estimated_tokens / self.context_size) * 100
            print(f"   ‚Ä¢ Uso del contexto: {usage_percent:.1f}%")
    
    def _stream_response(self, url, payload, enable_voice_streaming=False):
        """
        Maneja las respuestas en streaming
        
        Args:
            url (str): URL del endpoint
            payload (dict): Datos a enviar
            enable_voice_streaming (bool): Si True, activa TTS en paralelo
            
        Returns:
            str: Respuesta completa
        """
        try:
            response = requests.post(url, json=payload, stream=True)
            if response.status_code != 200:
                error_msg = f"Error: {response.status_code} - {response.text}"
                print(error_msg)
                return error_msg
            
            full_response = ""
            streaming_tts = None
            
            # Inicializar TTS streaming si est√° habilitado
            if enable_voice_streaming and self.voice_enabled:
                try:
                    streaming_tts = start_streaming_tts()
                    print("üó£Ô∏è TTS en paralelo activado")
                except Exception as e:
                    print(f"‚ùå Error al inicializar TTS streaming: {e}")
                    streaming_tts = None
            
            print("Respuesta del modelo (streaming):")
            try:
                for line in response.iter_lines():
                    if not line:
                        continue
                        
                    try:
                        data = json.loads(line.decode('utf-8'))
                    except json.JSONDecodeError as e:
                        print(f"\n‚ùå Error decodificando respuesta: {e}")
                        continue
                    
                    if 'error' in data:
                        error_msg = f"\n‚ùå Error del modelo: {data['error']}"
                        print(error_msg)
                        return error_msg
                    
                    if 'response' in data:
                        chunk = data['response']
                        if chunk:  # Solo procesar si hay contenido
                            print(chunk, end='', flush=True)
                            full_response += chunk
                            
                            # Enviar chunk al TTS en paralelo
                            if streaming_tts:
                                try:
                                    add_text_to_stream(chunk)
                                except Exception as e:
                                    print(f"\n‚ùå Error en TTS chunk: {e}")
                    
                    if data.get('done', False):
                        break
                        
            except Exception as e:
                print(f"\n‚ùå Error procesando respuesta streaming: {e}")
                return f"Error en streaming: {e}"
            
            # Finalizar TTS streaming
            if streaming_tts:
                try:
                    finish_streaming_tts()
                except Exception as e:
                    print(f"\n‚ùå Error al finalizar TTS streaming: {e}")
            
            if not full_response.strip():
                print("\n‚ö†Ô∏è La respuesta del modelo est√° vac√≠a")
                return "Sin respuesta"
                
            print()  # Nueva l√≠nea al final
            return full_response
            
        except requests.exceptions.RequestException as e:
            error_msg = f"Error de conexi√≥n: {e}"
            print(error_msg)
            return error_msg
    
    def chat(self):
        """
        Inicia una sesi√≥n de chat interactiva
        """
        print(f"ü§ñ Cliente de Ollama - Modelo: {self.model}")
        print(f"üß† Contexto: {self.context_size:,} tokens (como en terminal)")
        print("üé¨ Streaming: ACTIVADO por defecto")
        
        if self.voice_enabled:
            print("üé§üó£Ô∏è Modo VOZ: ACTIVADO")
        else:
            print("‚ö†Ô∏è Modo voz: NO disponible")
        
        print("Comandos disponibles:")
        print("  ‚Ä¢ 'salir' o 'exit' - Terminar sesi√≥n")
        print("  ‚Ä¢ 'stream' - Alternar modo streaming")
        print("  ‚Ä¢ 'historial' - Mostrar historial de conversaci√≥n")
        print("  ‚Ä¢ 'limpiar' - Limpiar historial")
        print("  ‚Ä¢ 'info' - Mostrar informaci√≥n de contexto")
        
        if self.voice_enabled:
            print("  ‚Ä¢ 'escuchar' - Activar entrada por voz")
            print("  ‚Ä¢ 'voz' - Alternar respuestas por voz")
            print("  ‚Ä¢ 'voz-streaming' - TTS en paralelo (recomendado)")
        
        print("-" * 60)
        
        use_stream = True  # Streaming activado por defecto
        use_voice_output = False  # Respuestas por voz
        use_voice_streaming = False  # TTS en paralelo
        
        while True:
            try:
                user_input = input("\nüë§ T√∫: ").strip()
                
                if user_input.lower() in ['salir', 'exit', 'quit']:
                    print("üëã ¬°Hasta luego!")
                    break
                
                if user_input.lower() == 'stream':
                    use_stream = not use_stream
                    status = "üé¨ ACTIVADO" if use_stream else "‚è∏Ô∏è DESACTIVADO"
                    print(f"‚úÖ Modo streaming: {status}")
                    continue
                
                if user_input.lower() in ['historial', 'history']:
                    self._show_history()
                    continue
                
                if user_input.lower() in ['limpiar', 'clear']:
                    self.clear_history()
                    continue
                
                if user_input.lower() == 'info':
                    self.show_context_info()
                    continue
                
                # Comandos de voz
                if self.voice_enabled and user_input.lower() in ['escuchar', 'listen']:
                    print("üé§ Modo escucha activado...")
                    voice_text = self.listen_to_voice()
                    if voice_text:
                        user_input = voice_text
                        print(f"üë§ T√∫ (por voz): {user_input}")
                    else:
                        print("üîá No se detect√≥ entrada de voz")
                        continue
                
                if self.voice_enabled and user_input.lower() in ['voz', 'voice']:
                    use_voice_output = not use_voice_output
                    status = "üó£Ô∏è ACTIVADA" if use_voice_output else "üîá DESACTIVADA"
                    print(f"‚úÖ Respuesta por voz: {status}")
                    if use_voice_output:
                        print("üí° Usa 'voz-streaming' para TTS en paralelo (m√°s fluido)")
                    continue
                
                if self.voice_enabled and user_input.lower() in ['voz-streaming', 'voice-streaming']:
                    use_voice_streaming = not use_voice_streaming
                    if use_voice_streaming:
                        use_voice_output = True  # Activar voz autom√°ticamente
                        status = "üé¨üó£Ô∏è ACTIVADO (TTS en paralelo)"
                    else:
                        status = "‚è∏Ô∏è DESACTIVADO"
                    print(f"‚úÖ TTS Streaming: {status}")
                    continue
                
                if not user_input:
                    continue
                
                print(f"\nü§ñ {self.model}:", end=" ")
                if use_stream:
                    # Si el streaming de voz est√° activado, no reproducir despu√©s
                    voice_after_stream = use_voice_output and not use_voice_streaming
                    response = self.generate_response(
                        user_input, 
                        stream=True, 
                        voice_streaming=use_voice_streaming
                    )
                    
                    # Solo reproducir voz despu√©s si no fue streaming
                    if voice_after_stream and response:
                        print("üó£Ô∏è Reproduciendo respuesta completa...")
                        self.speak_response(response)
                        
                else:
                    response = self.generate_response(user_input, stream=False)
                    print(response)
                    
                    # S√≠ntesis de voz si est√° activada (modo no-stream)
                    if use_voice_output and response and self.voice_enabled:
                        print("üó£Ô∏è Reproduciendo respuesta...")
                        self.speak_response(response)
                
            except KeyboardInterrupt:
                print("\nüëã ¬°Hasta luego!")
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")
    
    def _show_history(self):
        """
        Muestra el historial de conversaci√≥n
        """
        if not self.conversation_history:
            print("üìù No hay historial de conversaci√≥n")
            return
        
        print("üìù Historial de conversaci√≥n:")
        print("-" * 40)
        for i, entry in enumerate(self.conversation_history, 1):
            role_icon = "üë§" if entry['role'] == 'user' else "ü§ñ"
            role_name = "T√∫" if entry['role'] == 'user' else self.model
            content = entry['content'][:100] + "..." if len(entry['content']) > 100 else entry['content']
            print(f"{i}. {role_icon} {role_name}: {content}")

 
