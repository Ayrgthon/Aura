#!/usr/bin/env python3
"""
Script de demostraciÃ³n para Aura con soporte Gemini/Ollama + MCP
Muestra cÃ³mo usar ambos modelos con herramientas MCP
"""

import os
import asyncio
import warnings
from client import AuraClient

# Silenciar warnings
warnings.filterwarnings("ignore")

async def demo_gemini_with_mcp():
    """Demuestra Aura con Google Gemini y herramientas MCP"""
    print("ğŸŸ¢ === DEMO: Google Gemini con MCP ===")
    print("=" * 50)
    
    # Crear cliente Gemini
    client = AuraClient(
        model_type="gemini",
        model_name="gemini-2.0-flash-exp",
        enable_voice=False  # Sin voz para esta demo
    )
    
    # Configurar MCP
    home_dir = os.path.expanduser("~")
    allowed_dirs = [home_dir, "/tmp"]
    
    # Agregar directorios que existen
    for potential_dir in [f"{home_dir}/Documents", f"{home_dir}/Documentos", f"{home_dir}/Descargas"]:
        if os.path.exists(potential_dir):
            allowed_dirs.append(potential_dir)
    
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem"] + allowed_dirs,
            "transport": "stdio"
        }
    }
    
    print("ğŸ”§ Configurando servidores MCP...")
    mcp_success = await client.setup_mcp_servers(mcp_config)
    
    if not mcp_success:
        print("âš ï¸  MCP no disponible, continuando sin herramientas")
    
    # Pruebas con Gemini
    test_prompts = [
        "Hola, Â¿quiÃ©n eres?",
        "Lista los archivos en mi directorio home",
        "Â¿Puedes explicarme quÃ© es Python en 2 lÃ­neas?",
        "MuÃ©strame informaciÃ³n sobre un archivo llamado README.md si existe"
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n--- Prueba {i}: Gemini ---")
        print(f"ğŸ‘¤ Pregunta: {prompt}")
        print("ğŸ¤– Gemini:", end=" ")
        
        try:
            await client.chat_with_voice(prompt)
        except Exception as e:
            print(f"âŒ Error: {e}")
    
    return client

async def demo_ollama_with_mcp():
    """Demuestra Aura con Ollama y herramientas MCP"""
    print("\n\nğŸ¦™ === DEMO: Ollama con MCP ===")
    print("=" * 50)
    
    # Crear cliente Ollama
    try:
        client = AuraClient(
            model_type="ollama",
            model_name="qwen2.5-coder:7b",
            enable_voice=False  # Sin voz para esta demo
        )
    except Exception as e:
        print(f"âŒ Error inicializando Ollama: {e}")
        print("ğŸ’¡ AsegÃºrate de que Ollama estÃ© ejecutÃ¡ndose con: ollama serve")
        print("ğŸ’¡ Y que tengas el modelo instalado: ollama pull qwen2.5-coder:7b")
        return None
    
    # Configurar MCP (reutilizar la misma configuraciÃ³n)
    home_dir = os.path.expanduser("~")
    allowed_dirs = [home_dir, "/tmp"]
    
    for potential_dir in [f"{home_dir}/Documents", f"{home_dir}/Documentos", f"{home_dir}/Descargas"]:
        if os.path.exists(potential_dir):
            allowed_dirs.append(potential_dir)
    
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem"] + allowed_dirs,
            "transport": "stdio"
        }
    }
    
    print("ğŸ”§ Configurando servidores MCP para Ollama...")
    mcp_success = await client.setup_mcp_servers(mcp_config)
    
    if not mcp_success:
        print("âš ï¸  MCP no disponible, continuando sin herramientas")
    
    # Pruebas con Ollama (enfoque en programaciÃ³n)
    test_prompts = [
        "Hola, soy un desarrollador. Â¿Puedes ayudarme?",
        "Lista archivos Python en mi directorio si hay alguno",
        "Explica las diferencias entre async y sync en Python",
        "Â¿QuÃ© archivos de configuraciÃ³n encuentras en mi home?"
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n--- Prueba {i}: Ollama ---")
        print(f"ğŸ‘¤ Pregunta: {prompt}")
        print("ğŸ¦™ Ollama:", end=" ")
        
        try:
            await client.chat_with_voice(prompt)
        except Exception as e:
            print(f"âŒ Error: {e}")
    
    return client

async def comparison_demo():
    """Demuestra una comparaciÃ³n directa entre ambos modelos"""
    print("\n\nâš–ï¸  === DEMO: ComparaciÃ³n Gemini vs Ollama ===")
    print("=" * 60)
    
    # Pregunta comÃºn para ambos
    common_question = "Escribe una funciÃ³n Python que calcule el factorial de un nÃºmero de forma recursiva"
    
    print(f"ğŸ“ Pregunta comÃºn: {common_question}")
    print("\n" + "=" * 30 + " GEMINI " + "=" * 30)
    
    # Respuesta de Gemini
    try:
        gemini_client = AuraClient(model_type="gemini", enable_voice=False)
        print("ğŸ¤– Gemini:", end=" ")
        await gemini_client.chat_with_voice(common_question)
    except Exception as e:
        print(f"âŒ Error con Gemini: {e}")
    
    print("\n" + "=" * 30 + " OLLAMA " + "=" * 30)
    
    # Respuesta de Ollama
    try:
        ollama_client = AuraClient(model_type="ollama", enable_voice=False)
        print("ğŸ¦™ Ollama:", end=" ")
        await ollama_client.chat_with_voice(common_question)
    except Exception as e:
        print(f"âŒ Error con Ollama: {e}")

async def interactive_choice_demo():
    """Demo interactiva que permite al usuario elegir el modelo"""
    print("\n\nğŸ® === DEMO INTERACTIVA ===")
    print("=" * 40)
    print("Â¿QuÃ© modelo quieres probar?")
    print("1. ğŸŸ¢ Google Gemini")
    print("2. ğŸ¦™ Ollama")
    print("3. âš–ï¸  Comparar ambos")
    print("4. ğŸšª Salir")
    
    while True:
        try:
            choice = input("\nSelecciona una opciÃ³n (1-4): ").strip()
            
            if choice == "1":
                client = AuraClient(model_type="gemini", enable_voice=False)
                break
            elif choice == "2":
                client = AuraClient(model_type="ollama", enable_voice=False)
                break
            elif choice == "3":
                await comparison_demo()
                return
            elif choice == "4":
                print("ğŸ‘‹ Â¡Hasta luego!")
                return
            else:
                print("âŒ OpciÃ³n no vÃ¡lida")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Â¡Hasta luego!")
            return
    
    # Configurar MCP para el modelo elegido
    home_dir = os.path.expanduser("~")
    allowed_dirs = [home_dir, "/tmp"]
    
    for potential_dir in [f"{home_dir}/Documents", f"{home_dir}/Documentos", f"{home_dir}/Descargas"]:
        if os.path.exists(potential_dir):
            allowed_dirs.append(potential_dir)
    
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem"] + allowed_dirs,
            "transport": "stdio"
        }
    }
    
    print("ğŸ”§ Configurando MCP...")
    await client.setup_mcp_servers(mcp_config)
    
    # Chat interactivo
    print(f"\nğŸ—£ï¸  Chat con {client.model_type.upper()}")
    print("Escribe 'salir' para terminar")
    print("-" * 40)
    
    while True:
        try:
            user_input = input(f"\nğŸ‘¤ TÃº: ").strip()
            
            if user_input.lower() in ['salir', 'exit', 'quit']:
                break
            
            if not user_input:
                continue
            
            print(f"ğŸ¤– {client.model_type.upper()}:", end=" ")
            await client.chat_with_voice(user_input)
            
        except KeyboardInterrupt:
            break
    
    print("ğŸ‘‹ Â¡Hasta luego!")

async def main():
    """FunciÃ³n principal del demo"""
    print("ğŸŒŸ AURA - Demo de Capacidades Gemini/Ollama + MCP")
    print("=" * 60)
    
    # Verificar dependencias
    print("ğŸ” Verificando dependencias...")
    
    try:
        # Verificar imports bÃ¡sicos
        from langchain_google_genai import ChatGoogleGenerativeAI
        print("âœ… Google Gemini disponible")
    except ImportError:
        print("âŒ Google Gemini no disponible")
    
    try:
        from langchain_ollama import ChatOllama
        print("âœ… Ollama disponible")
    except ImportError:
        print("âŒ Ollama no disponible")
    
    try:
        from langchain_mcp_adapters.client import MultiServerMCPClient
        print("âœ… MCP adapters disponibles")
    except ImportError:
        print("âŒ MCP adapters no disponibles")
    
    print("\nÂ¿QuÃ© demo quieres ejecutar?")
    print("1. ğŸŸ¢ Demo completo con Gemini")
    print("2. ğŸ¦™ Demo completo con Ollama")
    print("3. âš–ï¸  ComparaciÃ³n lado a lado")
    print("4. ğŸ® Demo interactivo (tÃº eliges)")
    print("5. ğŸšª Salir")
    
    while True:
        try:
            choice = input("\nSelecciona una opciÃ³n (1-5): ").strip()
            
            if choice == "1":
                await demo_gemini_with_mcp()
                break
            elif choice == "2":
                await demo_ollama_with_mcp()
                break
            elif choice == "3":
                await comparison_demo()
                break
            elif choice == "4":
                await interactive_choice_demo()
                break
            elif choice == "5":
                print("ğŸ‘‹ Â¡Hasta luego!")
                break
            else:
                print("âŒ OpciÃ³n no vÃ¡lida")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Â¡Hasta luego!")
            break

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Â¡Hasta luego!")
    except Exception as e:
        print(f"âŒ Error: {e}") 